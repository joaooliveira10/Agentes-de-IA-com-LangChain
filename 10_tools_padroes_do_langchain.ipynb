{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools padrão do LangChain\n",
    "\n",
    "A biblioteca LangChain já possui diversas ferramentas padrão construídas que podemos utilizar. Elas podem ser verificadas neste link: https://python.langchain.com/docs/integrations/tools/.\n",
    "Vamos mostrar como utilizar algumas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ArXiv\n",
    "\n",
    "Esta ferramenta utiliza a biblioteca do arXiv para retornar resumos de artigos científicos de um determinado tema solicitado.\n",
    "\n",
    "https://python.langchain.com/v0.1/docs/integrations/tools/arxiv/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temos três formas em geral para chamar uma tool, da mais baixo a mais alto nível:\n",
    "- Criando a tool utilziando o Wrapper: no LangChain foram criados diferentes wrappers para apis e bibliotecas externas que facilitam a utilização das mesmas e permitem uma rápida criação de uma nova tool. Ela é mais customizável, pois podemos modificar as descrições e argumentos da tool às nossa necessidades, além de podermos modificar facilmente os parâmetros do wrapper\n",
    "- Instanciando a tool já criada: em geral, uma tool pronta já está criada dentro da biblioteca e podemos acessá-la diretamente\n",
    "- Utilizando o load_tools: o LangChain possui uma ferramnta especial chamada load_tools que permite o carregamento de diversas ferramentas ao mesmo tempo\n",
    "\n",
    "Vamos explorar os três métodos agora:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando tool manualmente através do Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities.arxiv import ArxivAPIWrapper\n",
    "from pydantic import BaseModel, Field #Importação atualizada\n",
    "from langchain.tools import StructuredTool\n",
    "\n",
    "class ArxivArgs(BaseModel):\n",
    "    query:str = Field(description='Qyuery de busca no ArXiv')\n",
    "\n",
    "tool_arxiv = StructuredTool.from_function(\n",
    "    func=ArxivAPIWrapper(top_k_results=2).run,\n",
    "    args_schema=ArxivArgs,\n",
    "    name='arxiv',\n",
    "    description = (\n",
    "        \"Uma ferramenta em torno do Arxiv.org. \"\n",
    "        \"Útil para quando você precisa responder a perguntas sobre Física, Matemática, \"\n",
    "        \"Ciência da Computação, Biologia Quantitativa, Finanças Quantitativas, Estatística, \"\n",
    "        \"Engenharia Elétrica e Economia utilizando artigos científicos do arxiv.org. \"\n",
    "        \"A entrada deve ser uma consulta de pesquisa em inglês.\"\n",
    "    ),\n",
    "    return_direct=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descrição: Uma ferramenta em torno do Arxiv.org. Útil para quando você precisa responder a perguntas sobre Física, Matemática, Ciência da Computação, Biologia Quantitativa, Finanças Quantitativas, Estatística, Engenharia Elétrica e Economia utilizando artigos científicos do arxiv.org. A entrada deve ser uma consulta de pesquisa em inglês.\n",
      "Args: {'query': {'description': 'Qyuery de busca no ArXiv', 'title': 'Query', 'type': 'string'}}\n"
     ]
    }
   ],
   "source": [
    "print('Descrição:', tool_arxiv.description)\n",
    "print('Args:', tool_arxiv.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Published: 2024-12-23\\nTitle: Trustworthy and Efficient LLMs Meet Databases\\nAuthors: Kyoungmin Kim, Anastasia Ailamaki\\nSummary: In the rapidly evolving AI era with large language models (LLMs) at the core,\\nmaking LLMs more trustworthy and efficient, especially in output generation\\n(inference), has gained significant attention. This is to reduce plausible but\\nfaulty LLM outputs (a.k.a hallucinations) and meet the highly increased\\ninference demands. This tutorial explores such efforts and makes them\\ntransparent to the database community. Understanding these efforts is essential\\nin harnessing LLMs in database tasks and adapting database techniques to LLMs.\\nFurthermore, we delve into the synergy between LLMs and databases, highlighting\\nnew opportunities and challenges in their intersection. This tutorial aims to\\nshare with database researchers and practitioners essential concepts and\\nstrategies around LLMs, reduce the unfamiliarity of LLMs, and inspire joining\\nin the intersection between LLMs and databases.\\n\\nPublished: 2024-06-13\\nTitle: Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications\\nAuthors: Irene Weber\\nSummary: Large Language Models (LLMs) have become widely adopted recently. Research\\nexplores their use both as autonomous agents and as tools for software\\nengineering. LLM-integrated applications, on the other hand, are software\\nsystems that leverage an LLM to perform tasks that would otherwise be\\nimpossible or require significant coding effort. While LLM-integrated\\napplication engineering is emerging as new discipline, its terminology,\\nconcepts and methods need to be established. This study provides a taxonomy for\\nLLM-integrated applications, offering a framework for analyzing and describing\\nthese systems. It also demonstrates various ways to utilize LLMs in\\napplications, as well as options for implementing such integrations.\\n  Following established methods, we analyze a sample of recent LLM-integrated\\napplications to identify relevant dimensions. We evaluate the taxonomy by\\napplying it to additional cases. This review shows that applications integrate\\nLLMs in numerous ways for various purposes. Frequently, they comprise multiple\\nLLM integrations, which we term ``LLM components''. To gain a clear\\nunderstanding of an application's architecture, we examine each LLM component\\nseparately. We identify thirteen dimensions along which to characterize an LLM\\ncomponent, including the LLM skills leveraged, the format of the output, and\\nmore. LLM-integrated applications are described as combinations of their LLM\\ncomponents. We suggest a concise representation using feature vectors for\\nvisualization.\\n  The taxonomy is effective for describing LLM-integrated applications. It can\\ncontribute to theory building in the nascent field of LLM-integrated\\napplication engineering and aid in developing such systems. Researchers and\\npractitioners explore numerous creative ways to leverage LLMs in applications.\\nThough challenges persist, integrating LLMs may revolutionize the way software\\nsystems are built.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_arxiv.run({'query': 'llm'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instanciando tool já criada no LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.arxiv.tool import ArxivQueryRun\n",
    "\n",
    "tool_arxiv = ArxivQueryRun(api_wrapper=ArxivAPIWrapper(top_k_results=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descrição: A wrapper around Arxiv.org Useful for when you need to answer questions about Physics, Mathematics, Computer Science, Quantitative Biology, Quantitative Finance, Statistics, Electrical Engineering, and Economics from scientific articles on arxiv.org. Input should be a search query.\n",
      "Args: {'query': {'description': 'search query to look up', 'title': 'Query', 'type': 'string'}}\n"
     ]
    }
   ],
   "source": [
    "print('Descrição:', tool_arxiv.description)\n",
    "print('Args:', tool_arxiv.args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando tool através do load_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ArxivQueryRun(api_wrapper=ArxivAPIWrapper(arxiv_search=<class 'arxiv.Search'>, arxiv_exceptions=(<class 'arxiv.ArxivError'>, <class 'arxiv.UnexpectedEmptyPageError'>, <class 'arxiv.HTTPError'>), top_k_results=3, ARXIV_MAX_QUERY_LENGTH=300, continue_on_failure=False, load_max_docs=100, load_all_available_meta=False, doc_content_chars_max=4000))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools = load_tools(['arxiv'])\n",
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ArxivQueryRun(api_wrapper=ArxivAPIWrapper(arxiv_search=<class 'arxiv.Search'>, arxiv_exceptions=(<class 'arxiv.ArxivError'>, <class 'arxiv.UnexpectedEmptyPageError'>, <class 'arxiv.HTTPError'>), top_k_results=3, ARXIV_MAX_QUERY_LENGTH=300, continue_on_failure=False, load_max_docs=100, load_all_available_meta=False, doc_content_chars_max=4000))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_arxiv = tools[0]\n",
    "tool_arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descrição: A wrapper around Arxiv.org Useful for when you need to answer questions about Physics, Mathematics, Computer Science, Quantitative Biology, Quantitative Finance, Statistics, Electrical Engineering, and Economics from scientific articles on arxiv.org. Input should be a search query.\n",
      "Args: {'query': {'description': 'search query to look up', 'title': 'Query', 'type': 'string'}}\n"
     ]
    }
   ],
   "source": [
    "print('Descrição:', tool_arxiv.description)\n",
    "print('Args:', tool_arxiv.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Published: 2024-12-23\\nTitle: Trustworthy and Efficient LLMs Meet Databases\\nAuthors: Kyoungmin Kim, Anastasia Ailamaki\\nSummary: In the rapidly evolving AI era with large language models (LLMs) at the core,\\nmaking LLMs more trustworthy and efficient, especially in output generation\\n(inference), has gained significant attention. This is to reduce plausible but\\nfaulty LLM outputs (a.k.a hallucinations) and meet the highly increased\\ninference demands. This tutorial explores such efforts and makes them\\ntransparent to the database community. Understanding these efforts is essential\\nin harnessing LLMs in database tasks and adapting database techniques to LLMs.\\nFurthermore, we delve into the synergy between LLMs and databases, highlighting\\nnew opportunities and challenges in their intersection. This tutorial aims to\\nshare with database researchers and practitioners essential concepts and\\nstrategies around LLMs, reduce the unfamiliarity of LLMs, and inspire joining\\nin the intersection between LLMs and databases.\\n\\nPublished: 2024-06-13\\nTitle: Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications\\nAuthors: Irene Weber\\nSummary: Large Language Models (LLMs) have become widely adopted recently. Research\\nexplores their use both as autonomous agents and as tools for software\\nengineering. LLM-integrated applications, on the other hand, are software\\nsystems that leverage an LLM to perform tasks that would otherwise be\\nimpossible or require significant coding effort. While LLM-integrated\\napplication engineering is emerging as new discipline, its terminology,\\nconcepts and methods need to be established. This study provides a taxonomy for\\nLLM-integrated applications, offering a framework for analyzing and describing\\nthese systems. It also demonstrates various ways to utilize LLMs in\\napplications, as well as options for implementing such integrations.\\n  Following established methods, we analyze a sample of recent LLM-integrated\\napplications to identify relevant dimensions. We evaluate the taxonomy by\\napplying it to additional cases. This review shows that applications integrate\\nLLMs in numerous ways for various purposes. Frequently, they comprise multiple\\nLLM integrations, which we term ``LLM components''. To gain a clear\\nunderstanding of an application's architecture, we examine each LLM component\\nseparately. We identify thirteen dimensions along which to characterize an LLM\\ncomponent, including the LLM skills leveraged, the format of the output, and\\nmore. LLM-integrated applications are described as combinations of their LLM\\ncomponents. We suggest a concise representation using feature vectors for\\nvisualization.\\n  The taxonomy is effective for describing LLM-integrated applications. It can\\ncontribute to theory building in the nascent field of LLM-integrated\\napplication engineering and aid in developing such systems. Researchers and\\npractitioners explore numerous creative ways to leverage LLMs in applications.\\nThough challenges persist, integrating LLMs may revolutionize the way software\\nsystems are built.\\n\\nPublished: 2024-05-30\\nTitle: Parrot: Efficient Serving of LLM-based Applications with Semantic Variable\\nAuthors: Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu\\nSummary: The rise of large language models (LLMs) has enabled LLM-based applications\\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\\nstrength of LLM and conventional software. Diverse LLM applications from\\ndifferent tenants could design complex workflows using multiple LLM requests to\\naccomplish one task. However, they have to use the over-simplified\\nrequest-level API provided by today's public LLM services, losing essential\\napplication-level information. Public LLM services have to blindly optimize\\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\\napplications.\\n  This paper introduces Parrot, an LLM service system that focuses on the\\nend-to-end experience of \""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_arxiv.run({'query': 'llm'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python REPL\n",
    "\n",
    "https://python.langchain.com/v0.1/docs/integrations/tools/python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descrição: A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\n",
      "Args: {'query': {'title': 'Query', 'type': 'string'}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
    "\n",
    "tool_repl = PythonREPLTool()\n",
    "print('Descrição:', tool_repl.description)\n",
    "print('Args:', tool_repl.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python REPL can execute arbitrary code. Use with caution.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'oi\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_repl.run({'query': 'print(\"oi\")'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StackOverflow\n",
    "\n",
    "https://python.langchain.com/v0.1/docs/integrations/tools/stackexchange/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descrição: A wrapper around StackExchange. Useful for when you need to answer specific programming questionscode excerpts, code examples and solutionsInput should be a fully formed question.\n",
      "Args: {'query': {'title': 'Query', 'type': 'string'}}\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import load_tools\n",
    "\n",
    "tools = load_tools(['stackexchange'])\n",
    "tool_stack = tools[0]\n",
    "print('Descrição:', tool_stack.description)\n",
    "print('Args:', tool_stack.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Question: Why is my agent not retaining past conversations?\\nHere is my current code setup:\\nimport type { BaseChatModel } from &quot;@<span class=\"highlight\">langchain</span>/core/language_models/chat_models&quot;;\\nimport { ChatGoogleGenerativeAI } from &quot;@<span class=\"highlight\">langchain</span>/google-genai&quot;;\\nimport {\\n  MemorySaver &hellip; ,\\n} from &quot;@<span class=\"highlight\">langchain</span>/langgraph&quot;;\\nimport {\\n  createReactAgent,\\n} from &quot;@<span class=\"highlight\">langchain</span>/langgraph/prebuilt&quot;;\\nimport type { Tool } from &quot;<span class=\"highlight\">langchain</span>/tools&quot;;\\n\\n\\n\\ninterface AgentInfo{\\n  id: string,\\n  name: string, &hellip; \\n\\nQuestion: Where are LangChain&#39;s chat templates stored?\\nYou know how when you use chains or <span class=\"highlight\">agents</span>, <span class=\"highlight\">LangChain</span> automatically adds all the necessary chat templates?\\nWell, I decided to look for them, to see how they actually look. &hellip; \\n\\nQuestion: Getting OpenAI API Error While Using Groq API for Python Coding Agent\\nI am trying to create a Python coding <span class=\"highlight\">agent</span> using the crewai library along with <span class=\"highlight\">langchain</span> and ChatGroq. &hellip; =DebuggerAgent,\\n    llm=llm,\\n    # output_file=&#39;temp.py&#39;  # Example of output customization\\n)\\n\\n# Forming the tech-focused crew with some enhanced configurations\\ncrew = Crew(\\n    <span class=\"highlight\">agents</span>=[coderAgent, DebuggerAgent &hellip; '"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_stack.run({'query': 'LangChain Agents'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File System\n",
    "\n",
    "https://python.langchain.com/v0.1/docs/integrations/tools/filesystem/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: copy_file\n",
      "Descrição: Create a copy of a file in a specified location\n",
      "Args: {'source_path': {'description': 'Path of the file to copy', 'title': 'Source Path', 'type': 'string'}, 'destination_path': {'description': 'Path to save the copied file', 'title': 'Destination Path', 'type': 'string'}}\n",
      "\n",
      "Name: file_delete\n",
      "Descrição: Delete a file\n",
      "Args: {'file_path': {'description': 'Path of the file to delete', 'title': 'File Path', 'type': 'string'}}\n",
      "\n",
      "Name: file_search\n",
      "Descrição: Recursively search for files in a subdirectory that match the regex pattern\n",
      "Args: {'dir_path': {'default': '.', 'description': 'Subdirectory to search in.', 'title': 'Dir Path', 'type': 'string'}, 'pattern': {'description': 'Unix shell regex, where * matches everything.', 'title': 'Pattern', 'type': 'string'}}\n",
      "\n",
      "Name: move_file\n",
      "Descrição: Move or rename a file from one location to another\n",
      "Args: {'source_path': {'description': 'Path of the file to move', 'title': 'Source Path', 'type': 'string'}, 'destination_path': {'description': 'New path for the moved file', 'title': 'Destination Path', 'type': 'string'}}\n",
      "\n",
      "Name: read_file\n",
      "Descrição: Read file from disk\n",
      "Args: {'file_path': {'description': 'name of file', 'title': 'File Path', 'type': 'string'}}\n",
      "\n",
      "Name: write_file\n",
      "Descrição: Write file to disk\n",
      "Args: {'file_path': {'description': 'name of file', 'title': 'File Path', 'type': 'string'}, 'text': {'description': 'text to write to file', 'title': 'Text', 'type': 'string'}, 'append': {'default': False, 'description': 'Whether to append to an existing file.', 'title': 'Append', 'type': 'boolean'}}\n",
      "\n",
      "Name: list_directory\n",
      "Descrição: List files and directories in a specified folder\n",
      "Args: {'dir_path': {'default': '.', 'description': 'Subdirectory to list.', 'title': 'Dir Path', 'type': 'string'}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.agent_toolkits.file_management.toolkit import FileManagementToolkit\n",
    "\n",
    "tool_kit = FileManagementToolkit(\n",
    "    root_dir='arquivos'\n",
    ")\n",
    "tools = tool_kit.get_tools()\n",
    "for tool in tools:\n",
    "    print('Name:', tool.name)\n",
    "    print('Descrição:', tool.description)\n",
    "    print('Args:', tool.args) \n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dando um exemplo de aplicação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.agent_toolkits.file_management.toolkit import FileManagementToolkit\n",
    "\n",
    "tool_kit = FileManagementToolkit(\n",
    "    root_dir='arquivos',\n",
    "    selected_tools=['write_file', 'read_file', 'file_search','list_directory']\n",
    ")\n",
    "tools = tool_kit.get_tools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "\n",
    "tools_json = [convert_to_openai_function(tool) for tool in tools]\n",
    "tool_run = {tool.name: tool for tool in tools}\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'Você é um assistente amigável chamado Isaac capaz de gerenciar arquivos'),\n",
    "    ('user', '{input}')\n",
    "])\n",
    "chat = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.agents import AgentFinish #Importação do AgentFinish atualizada\n",
    "\n",
    "def roteamento(resultado):\n",
    "    if isinstance(resultado, AgentFinish):\n",
    "        return resultado.return_values['output']\n",
    "    else:\n",
    "        return tool_run[resultado.tool].run(resultado.tool_input)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\n",
    "\n",
    "\n",
    "chain = prompt | chat.bind(functions=tools_json) | OpenAIFunctionsAgentOutputParser() | roteamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Olá! Eu sou o assistente Isaac e sou capaz de gerenciar arquivos. Posso ler, escrever, procurar e listar arquivos em um diretório. Como posso te ajudar hoje?'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'input': 'O que você é capz de fazer?'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Explorando o Universo das IAs com Hugging Face.pdf'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'input': 'Quais arquivos pdf você tem na pasta?'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Error: [Errno 2] No such file or directory: '/home/adriano/Documentos/Asimov/asimov_academy/ensino/cursos/courses/Agents de IA com Python e LangChain/v2/scripts/arquivos/files'\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'input': 'Quais arquivos txt você tem na pasta?'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'File written successfully to notas.txt.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'input': 'Crie um arquivo txt chamado notas com o seguinte conteúdo \"Isso foi feito por uma LLM\"'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Isso foi feito por uma LLM'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'input': 'Leia o arquivo notas.txt'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
